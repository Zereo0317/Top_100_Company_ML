# SVMæ¨¡å‹è¨“ç·´èˆ‡ç‰¹å¾µèª¿æ•´

### ä»‹ç´¹å¦‚ä½•ä½¿ç”¨æ”¯æŒå‘é‡æ©Ÿï¼ˆSVMï¼‰æ¨¡å‹é€²è¡Œè¨“ç·´ï¼Œä¸¦è¨ˆç®—æ¨¡å‹æ±ºç­–é‚Šç•Œï¼Œåˆ¤æ–·ç‰¹å¾µè³‡æ–™ä¸­çš„å…¬å¸ç‰¹å¾µåšä»€éº¼æ¨£çš„è®Šå‹•ï¼Œèƒ½è®“å…¬å¸æœ‰æ©Ÿæœƒé€²å‰ç™¾å¤§ã€‚

## è®€å–è³‡æ–™

é¦–å…ˆï¼Œæˆ‘å€‘è®€å–äº†è¨“ç·´æ•¸æ“šå’Œæ¸¬è©¦æ•¸æ“šï¼Œä¸¦ç§»é™¤äº†ä¸å¿…è¦çš„æ¬„ä½ï¼ˆä¾‹å¦‚ 'N_name' å’Œ 'Name'ï¼‰ã€‚

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
import matplotlib.pyplot as plt

data = pd.read_csv('dataset/Top100_final.csv')
wanna_know_data = pd.read_csv('dataset/test_cody.csv')

data = data.drop(columns=['N_name', 'Name'])
wanna_know_data = wanna_know_data.drop(columns=['N_name', 'Name'])
```



## æ ¹æ“šå»ºæ¨¡æ™‚çš„ç¶“é©—(è©³æƒ…è«‹è¦‹SVM_Read.md)é¸æ“‡åƒæ•¸ä¸¦è¨“ç·´æ¨¡å‹
```python
# æ¨™æº–åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
wanna_know_data_X_scaled = scaler.transform(wanna_know_data_X)

# è¨“ç·´SVMæ¨¡å‹
svm_classifier = SVC(kernel='rbf', C=7, gamma=0.3, probability=True)
svm_classifier.fit(X_train_scaled, y_train)
```



## å°‹æ‰¾æ±ºç­–è¶…å¹³é¢ï¼Œæ§åˆ¶èª¿æ•´é‡
![ä¸‰ç¶­æ±ºç­–å¹³é¢ç¯„ä¾‹](decision_plane.png "ä¸‰ç¶­æ±ºç­–å¹³é¢ç¯„ä¾‹")
- å‘é‡æ©Ÿçš„æ±ºç­–å‡½æ•¸é€šå¸¸ç‚ºğ‘“(ğ‘¥)=ğ‘¤â‹…ğ‘¥+ğ‘
    - ğ‘¤æ˜¯åˆ†éš”è¶…å¹³é¢çš„æ³•å‘é‡ 
    - xæ˜¯ç‰¹å¾µå‘é‡(è¶…ç©ºé–“ä¸­ä¸€å€‹é») 
    - bæ˜¯æˆªè·
- æ±ºç­–å‡½æ•¸è‹¥returnè² å€¼ï¼Œè¡¨ç¤ºæ­¤ç‰¹å¾µæ˜¯åä¾‹(ä¸æ˜¯Top100çš„é‚£å´)
- æ¢¯åº¦ âˆ‡ğ¿ è¡¨ç¤ºç›®æ¨™å‡½æ•¸ ğ¿ å°æ–¼æ¨¡å‹åƒæ•¸çš„è®ŠåŒ–ç‡ï¼Œç²—ç•¥ä¾†èªªå°±æ˜¯å° ğ¿ å¾®åˆ†ã€‚dual_coefå¯ä»¥å¹«æˆ‘å€‘å¾—åˆ° ğ‘¤ çš„åå¾®åˆ†

```python
# è¨ˆç®—é æ¸¬é›†ä¸­æ¯å€‹æ¨£æœ¬çš„æ±ºç­–å‡½æ•¸å€¼
decision_values = svm_classifier.decision_function(wanna_know_data_X_scaled)

# æ‰¾å‡ºéœ€è¦èª¿æ•´çš„æ¨£æœ¬ï¼ˆå‡è¨­æˆ‘å€‘æƒ³è¦å°‡æ‰€æœ‰ä¸æ˜¯Top_100çš„æ¨£æœ¬è®ŠæˆTop_100ï¼‰
samples_to_adjust = wanna_know_data_X_scaled[decision_values < 0]

# è¨ˆç®—æ¯å€‹ç‰¹å¾µçš„æ¢¯åº¦
gradients = np.dot(svm_classifier.dual_coef_, svm_classifier.support_vectors_).flatten()

scale_shrink = 0.01

# è¨ˆç®—èª¿æ•´é‡
adjustments = scale_shrink * gradients * -decision_values[decision_values < 0].reshape(-1, 1)
```

## çµæœæ•´ç†å’Œè¼¸å‡º
### Notice : ç”±æ–¼è¨“ç·´æ¨¡å‹æ˜¯å°ºç”¨é‚£å¨è¨“ç·´è³‡æ–™çš„StandardScaleræ¨™æº–åŒ–éçš„æ¨¡å‹,æ‰€ä»¥è¼¸å…¥éœ€ç‚ºè¢«åŒä¸€å€‹"æ¨™æº–åŒ–å™¨"çš„è³‡æ–™,åŒæ¨£åœ°,è½‰æ›å›å»ä¹Ÿè¦ç”¨ç›¸åŒçš„"æ¨™æº–åŒ–å™¨"Inverse Transform
### é€™è£¡å°±ä¸åšè³‡æ–™é€†æ¨™æº–åŒ–,ç›´æ¥å‘ˆç¾æ¨¡å‹èªªæ˜é€™æ¨£æ”¹è®Šçš„åŒ–å°±æœ‰æ©Ÿæœƒé€²Top 100
![](Adjust_Value.png "æ”¹è®Šå€¼")
```python
# è¨ˆç®—èª¿æ•´å¾Œçš„ç‰¹å¾µå€¼
adjusted_samples = samples_to_adjust + adjustments

# å°å‡ºèª¿æ•´å¾Œçš„ç‰¹å¾µå€¼
print("\nèª¿æ•´å¾Œçš„ç‰¹å¾µå€¼:")
print(pd.DataFrame(adjusted_samples, columns=wanna_know_data_X.columns))
```